<!DOCTYPE html>
<html lang="en">
  <head>    
    <link rel="stylesheet" href="css/documentation.css">
    <title>Documentation</title>
  </head>
    
  <body>

    <nav class="slidemenu">
      
      <!-- Item 1 -->
      <input type="radio" name="slideItem" id="slide-item-1" class="slide-toggle" />
      <label for="slide-item-1"><a href="/index.html"><p class="icon">üè†</p></a><span>Home</span></label>
      

      <!-- Item 3 -->
      <input type="radio" name="slideItem" id="slide-item-2" class="slide-toggle"/>
      <label for="slide-item-2"><a href="/workflow.html"><p class="icon">‚öôÔ∏è</p></a><span>Workflow</span></label>
      
      <!-- Item 4 -->
      <input type="radio" name="slideItem" id="slide-item-3" class="slide-toggle" checked/>
      <label for="slide-item-3"><a href="documentation.html"><p class="icon">üïÆ</p></a><span>Documentation</span></label>
      
      <!-- Item 4 -->
      <input type="radio" name="slideItem" id="slide-item-4" class="slide-toggle"/>
      <label for="slide-item-4"><a href="/visualization.html"><p class="icon"> üñ•Ô∏è</p></a><span>Visualization</span></label>
      <!-- Item 4 -->
      <input type="radio" name="slideItem" id="slide-item-5" class="slide-toggle"/>
      <label for="slide-item-5"><a href="/metadata.html"><p class="icon">üîç</p></a><span>Metadata</span></label>

      <input type="radio" name="slideItem" id="slide-item-6" class="slide-toggle"/>
      <label for="slide-item-6"><a href="/about.html"><p class="icon">ü§∑</p></a><span>About</span></label>
      <div class="clear"></div>
      
      <!-- Bar -->
      <div class="slider">
        <div class="bar"></div>
      </div>
      
    </nav>


    <h1>Documentation</h1>

    <ul>
      <li>1. Introduction</li>
      <li>2. Scenario</li>
      <li>3. Overview on datasets</li>
      <li>4. Analysis of original datasets
        <ul>
          <li>4.1 Quality analysis</li>
          <li>4.2 Legal analysis</li>
          <li>4.3 Ethical analysis</li>
          <li>4.4 Technical analysis</li>
        </ul>
      </li>
      <li>5. Mash-up and final datasets
        <ul>
          <li>5.1 Principles and goals</li>
          <li>5.2 Changes to original datasets and creation of final ones</li>
        </ul>
      </li>
      <li>6. Sostenibility over time</li>
      <li>7. Visualization
        <ul>
          <li>7.1 Visualization tools</li>
          <li>7.2 Visualizations for each dataset</li>
        </ul>
      </li>
      <li>8. RDF assertion of metadata</li>
      <li>9. Conclusions</li>
    </ul>

    <h2>1. Introduction</h2>
    <p>This project was developed as the final exam of the course Open Access and Digital Ethics within the master course degree Digital Humanities and Digital Knowledge, of the University of Bologna, held by professor Monica Palmirani.</p>
    <p>The aim of the project was to <b>develop effective visualizations</b> starting from an hypothesis of <b>re-use of datasets</b> free from cognitive biases, fair, legal valid, consistent, accurate.</p>
    <p>To achieve this goal, our workflow began with the definition of the main idea, thus the selection of the <b>application scenario</b>; followed by <b>data collection and data analysis</b>; to the <b>mash-up of collected data</b> to compose a new dataset whose data would then be displayed through graphs, maps and other <b>appropriate visualizations</b>.</p>
    <p>Source datasets have been collected from official and established European and international institutions and then analyzed from an ethical-legal and technical point of view. To manage a mash-up of different datasets with different licenses we have followed the Guidelines for Open Data provided by the EU, with particular regard to data curation. In these steps we had to analyze legal and ethics issues, economical and sustainable aspects, technical and metadata foundations; in order to produce accurate  metadata using DCAT_AP through RDF.</p>

    <h2>2. Scenario</h2>
    <h4>Is human trafficking related to the country well-being within an International Organisation?</h4>
    <p>In order to choose our scenario, we decided to select the type of data to publish according to the G8 Open Data Charter and European Commission guidelines (Open Data Goldbook for Data Managers and Data Holders by the European Data Portal). Indeed, some categories of data are more relevant than others and have higher potential values: companies, crime and justice, earth observation, education, energy and environment, finance and contracts, geospatial, global development, government accountability and democracy, health, science and research, statistics, social mobility and welfare, transport and infrastructures. For this reason we chose a scenario at the crossroads of different categories: human trafficking and its supposedly related factors, such as the well-being of a country. The main category of reference data is crime and justice, but secondly some data belongs to the categories of education, development and government.</p>
    <p>This project scenario could be explained more clearly through the 5 W:</p>
    <p>1) <b>WHAT</b>: human trafficking and supposedly related factors. We tried to investigate data, through data analysis and visualization, to try to understand if there were any patterns, trends, in the evolution of this crime over the years and especially in the individual states. Given the delicacy but also the complexity of the topic, we immediately asked ourselves whether human trafficking was linked to the well-being of a state, defined on the basis of criteria of economic development, individual happiness, gross domestic product and so on. It is commonplace to think that the states with the highest poverty rate are the states with the highest crime rate, but is this really the case? If so, are there any states where this correlation is more evident? These are just some of the questions we asked ourselves.</p>
    <p>2) <b>WHEN</b>: the 2000s, most data are between 2007 and 2016. We decided to choose this time range for two main reasons: firstly, the greater quantity and quality of data at our disposal regarding human trafficking as a crime, but also the greater quantity and quality of secondary or complementary data. Secondly, we chose a time range large enough to allow us to have an overview of the trend of this crime, but not too broad to prevent us from grasping significant patterns on a small scale.</p>
    <p>3) <b>WHERE</b>: the states of the European Union. Most of the datasets we found contain data on all states of the world or most of them. We were more interested in investigating this phenomenon within an international organization such as the European Union. This is for several reasons: firstly, to try to understand if there are national differences (or, better, how many differences and how big) despite having adhered to the same declarations of struggle in favor of human rights, and adhering to the same economic policies. Secondly, because when we talk about human trafficking we first think of the poorest countries in the world, when various sources argue that it is a crime widely supported by the richest nations of the continents, including Europe.</p>
    <p>4) <b>WHO</b> (the victims): age, sex, country of origin. Our focus was more on victims than on traffickers. We believe that individual characteristics of these people, such as their citizenship or their sex, linked to the forms of exploitation they have suffered, can give us more interesting insights into what are the reasons for trafficking in human beings, what are its networks and why certain places are more privileged than others.</p>
    <p>5) <b>WHY</b>: forms of exploitation, as they could be considered as the scope of human traffickers. Trafficking in human beings is a crime linked to different forms of exploitation: forced labor, child labor, forced prostitution, illegal international adoptions and much more. Identifying the links between all these crimes can certainly help fight them more effectively.</p>


    <h2>3. Overview on datasets</h2>
    <p>Given the scenario chosen for this project, we decided to select two broad categories of datasets: one that specifically concerns the trafficking of human beings, so we looked for datasets that contained data on victims, forms and places of exploitation; and the other that could instead give us an image of the nations of the European Union from an economic, social, growth and well-being point of view. We have therefore selected datasets of established international institutions and organizations:</p>
    <p><b>CTDC - Counter Trafficking Data Collaborative</b>: first global data hub on human trafficking, with data contributed by organizations from around the world (e.g.,IOM ‚Äì International Organization for Migration, Liberty Shared, Polaris Project). Launched in November 2017, the goal of CTDC is to break down information-sharing barriers and equip the counter-trafficking community with up to date, reliable data on human trafficking. It counts 108,613 individual cases collected, 164 countries of exploitation, 175 nationalities. Its web resource provides 376 resources, 24 datasets, 21 data story, 13 documents and 4 data dashboard.</p>
    <p><b>UNODC ‚Äì Office on Drugs and Crime of the United Nations</b>: UNODC research and data provider constitutes the key global authority in the fields of drugs and crime, providing high-quality, essential evidence to inform policy-making and valuable sources of knowledge in drugs and crime domains. The Thematic Program on Research, Trend Analysis and Forensics defines the key challenges, work priorities and quality standards, as well as managing global and regional data collections, provides scientific and forensic services, defines research standards, and supports Member States to strengthen their data collection, research and forensics capacity.</p>
    <p><b>EU Commission and EU Open Data Portal</b>: the European Union Commission, to address trafficking in human beings the EU, has put in place a comprehensive, gender-specific and victim-centred legal and policy framework and has developed a dedicated EU Anti-Trafficking resource called ‚ÄúTogether Against Trafficking in Human Beings‚Äù collecting Legal and Policy Framework, information about EU projects and funding, as well as publications and reports. Data on human trafficking are also available on the EU Open Data Portal and the European Data Portal.</p>
    <p><b>World Bank Group and World Bank Open Data</b>: provides free and open access to global development data. At the World Bank, the Development Data Group coordinates statistical and data work and maintains a number of macro, financial and sector databases. Working closely with the Bank‚Äôs regions and Global Practices, the group is guided by professional standards in the collection, compilation and dissemination of data to ensure that all data users can have confidence in the quality and integrity of the data produced.</p>
    <p><b>OECD ‚Äì Organisation for Economic Co-operation and Development</b>: international organisation that works to build better policies for better lives, having the goal of shaping policies that foster prosperity, equality, opportunity and well-being for all. Together with governments, policy makers and citizens, OECD works on establishing evidence-based international standards and finding solutions to a range of social, economic and environmental challenges. From improving economic performance and creating jobs to fostering strong education and fighting international tax evasion, OECD provides a unique forum and knowledge hub for data and analysis, exchange of experiences, best-practice sharing, and advice on public policies and international standard-setting. Among all these activities, OECD also collects statistical data on those indicators considered to be the signs of the wellness of a country.</p>
    <br>
    <p>To have more specific information regarding original dataset, you can either skip to section 4. Analysis of original datasets or go to the Metadata page of ODOHTEU web resource.</p>

    <p>TABBBEEEELLLAAA</p>

    <p>*we have collected 28 datasets from the WBG, one for each member states of the European Union (until 2018, thus considering also UK). For reasons of brevity and to avoid redundancy we have decided to identify them with a single dataset as they have the same identifying characteristics, variables, sources and organization.</p>
    <br>
    <p>However, some datasets found have greater geographical coverage or temporal coverage than those of our interest, or contain more indicators than those needed for our research. For this reason we found it necessary to manipulate the datasets to obtain what became the final ODOHTEU datasets. More information on this can be found in section "5. Mashup and final datasets".</p>
    <br>
    <p>FINAL DATASETS:</p>

    <p>TABBBEEEELLLAAA</p>



    <h2>4. Analysis of original datasets</h2>
    <p>A premise is necessary and fundamental to correctly understand and analyze the nature and completeness of the data collected: statistical information on the total number of victims of trafficking in human beings, as well as data regarding age, sex, forms of exploitation, citizenship and other valuable indicators are likely to be the most difficult data to collect, as stated by the European Union Commission (EU Data collection on trafficking in human  beings in the EU, 2020).</p>
    <p>To choose the best datasets to investigate the criminal phenomenon of human trafficking, we followed the guidelines of the Open Data Goldbook for Data Managers and Data Holders by the European Data Portal. In particular, for each dataset we asked ourselves:</p>
    <p>Can it be published (legally, politically, and organisationally)?</p>
    <p>Is it of the right quality (and thus does not need thorough manipulation before publication)?</p>
    <p>What about cleaning, anonymising, good quality and format?</p>
    <p>Does it belong to one of the high-value topics?</p>
    <p>While most of the datasets available on the topic belong to a high-value topic and can be published, not all of them have good quality and format; thus several modifications were made to obtain the final mash-up datasets.</p>

    <h3>4.1 Quality analysis</h3>
    <p>For the quality analysis of the original datasets we have found, we decided to rely on the standards defined by the Open Data Goldbook for Data Managers and Data Holders. Indeed, there are three main aspects of data quality: content quality, timeliness, and consistency.</p>

    <h4>4.1.1 Content Quality</h4>
    <p>Content quality concerns completeness, cleanness and accuracy of data.</p>
    <p><b>Completeness</b> can be evaluated by the presence of a header row with a single description of what is shown (and in the metadata the header should be described); by the label of a version number (to keep track of changes); by the presence of origin and scope information on data; and by a given status (draft, validated, final).</p>
    <p><b>Cleanness</b> concerns empty fields, dummy data and default values, wrong values, double entries and privacy sensitive information. In evaluating cleanness, we had to take in consideration that data about human trafficking are among the most difficult data to collect with certainty and completeness, both among all the data belonging to the category of crime and justice, and in general among all the collectable statistical data.</p>
    <p>Finally, <b>accuracy</b> concerns data purpose, reliability, the choices concerning interval described, and aggregation or disaggregation needs for data.</p>
    <p>TABBBEEELLLAAAAAAAAA</p>

    <br>

    <h4>4.1.2 Timeliness</h4>
    <p>Since data changes over time, especially recent data need to be updated over time. We then checked if datasets source institutions adopt an update process to keep data up-to-date and their data contains  a notion of its timeliness.</p>
    <p>TABBBEEELLLAAAAAAAAA</p>


    <h4>4.1.3 Consistency</h4>
    <p>When dealing with consistency of the presentation of data, we can relate to its accuracy, reliability, timeliness and all those aspects that allow for a simple and correct re-use of data without the need of manipulating them (e.g., the same variables ‚Äì header row names ‚Äì are used in different version of the same dataset). Consistency also deals with coherence: are data in the datasets organized accordingly to the specification provided by the source institution?</p>
    <p>TABBBEEELLLAAAAAAAAA</p>



    <h3>4.2 Legal analysis</h3>
    <p>For the legal analysis of our dataset, we have taken into account the standards defined both by the Open Data Goldbook for Data Managers and Data Holders, and four main directives for the Open Data release:</p>
    <p>1) Privacy: GDPR Regulation (EU) 2016/679, Regulation (EU) 2018/1807, Directive 2002/58/EC;</p>
    <p>2) PSI: Directive (EU) 2019/1024;</p>
    <p>3) CDSM: DIRECTIVE (EU) 2019/790;</p>
    <p>4) INSPIRE: Directive 2007/2/EC that define particular limitation on public access for the spatial and geo data.</p>
    <p>Given these standards chosen for the analysis, the main criteria according to which the legal analysis of the datasets was carried out are: <b>privacy issues, licesing, legislation accordance, intellectual property rights, privacy issues, liability, commercial law, limitations on public access, economical conditions and temporary aspects</b>.</p>

    <h4>4.2.1 Legal analysis key points: issues, privacy, accountability and licences</h4>
    <p>Historically, it has been difficult to make data on human trafficking readily accessible to analysts, academics, practitioners and policy-makers. Data on human trafficking are often highly sensitive raising a range of privacy and civil liberty concerns where the risk of identifying data subjects can be high and the consequences severe, as stated by CTDC (cita sito).
    It is widely recognized that one of the foremost challenges in developing targeted counter-trafficking responses and measuring their impact is the lack of reliable, high-quality information. Data on human trafficking are often highly sensitive, raising a range of privacy and civil liberty concerns where the risk of identifying data subjects can be high and the consequences severe.</p>
    <p>In the case of the ODOHTEU project, some datasets found and reused have in fact chosen some data anonymization techniques (such as CTDC, which uses k-anonymization for the dataset, even if non k-anonymized version of the dataset is displayed throughout the website through visualizations and charts showing detailed analysis). Other datasets do not contain information on individual victims identified but rather national or overall data (often as a percentage) of all the states of the entire European Union, thus making it very difficult to recognize individuals, since data are disaggregated at the level of the individual.</p>
    <p>In all of the dataset there is no personal data as defined in the GDPR (i.e., data including information such as individuals name, address, ID card/passport number, income, cultural profile, Internet Protocol (IP) address, data about health). However, four datasets have data considered special categories of data, and in particular the citizenship of the individuals detected. Those datasets are: CTDC global dataset, UNODC victim citizenship, EU report 2018 and EU report 2020. Yet, CTDC uses k-anonymization, while the other three datasets show the total of individuals with a specific citizenship by exploitation nation, and this information is never crossed in the same dataset with other information that could be considered sensitive or personal, such as gender or age.</p>
    <p>For what concerns privacy issues and accountability in general, it should be borne in mind that some organizations such as UNODC and CTDC delegate some responsibilities to individual country authorities that have provided data on victims, or to organizations that are partially data sources (for example the Polaris project for CTDC).</p>
    <p>As regards the licenses, half of the datasets are under <b>CC BY 4.0 license</b> (World Bank data and EU data), while the institutions that own the other half of the datasets all allow for use, download, copy, adapt, print data as long ad the source institution is cited, either for non-commercial use only (CTDC) or even for commercial use (OECD).</p>

    <h4>4.2.2 Legal analysis of each dataset</h4>
    <p>TABELLA MOSTRUOSA</p>
    <p>Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of information society services, in particular electronic commerce, in the Internal Market ('Directive on electronic commerce') - <a href="http://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32000L0031">http://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32000L0031</a></p>



    <h3>4.3 Ethical analysis</h3>
    <p>For what concern the ethical analysis of the original datasets, we followed the principles proposed by Data Ethics: Principles and Guidelines for Companies, Authorities & Organisations. Indeed, to evaluate our datasets we took in consideration the following criteria: the human being at the center; individual data control; transparency; accountability, equality. We also checked if data are bias-free and sustainable.</p>

    <p>Broadly speaking, most of the institutions or organization owning the datasets used for ODOHTEU project value more principles such as <b>transparency and accountability</b>, while not all put the <b>human being at the center</b> in data processing, as well as for individual data control: for example, almost none of the institutions provide a specific and dedicated method for collecting request of deletion of personal data (eg right to be forgotten). More attention is given to <b>sustainability</b> by those data providers who guarantee accuracy and clarity in updating data, and also explicitly declare the temporary policy for updating the datasets. Given the delicacy and sensitivity of the type of data that are processed by most of the institutions from which we have collected the data, it is absolutely impossible for them to exclude the ethical principles of <b>equity and bias-free</b>. Some data providers and owners have stood out among others not only for the attention to respect for ethical principles, but also for the clarity in expressing and telling them to the user, as well as in making them easily available for anyone wishing to reuse their datasets consciously. responsibly. These are the <b>World Bank Group and CTDC</b>.</p>

    <p><b>CTDC</b>: for CTDC the <b>human being at the center</b> is one of the core principles of their work. As it is stated in CTDC web resource, ‚ÄúCounter-trafficking case data contains highly sensitive information, and maintaining privacy and confidentiality is of paramount importance for CTDC.¬†For example, all explicit identifiers, such as names, were removed from the global victim dataset and some data such as age has been transformed into age ranges. No personally identifying information is transferred to or hosted by CTDC, and organizations that want to contribute are asked to anonymize in accordance to the standards set by CTDC.‚Äù Since IOM is one of the main contributors to CTDC database, as well as the founder of the CTDC initiative, we have to consider also IOM‚Äôs Data Protection Policy for data collection and data process. CTDC and its contributors are clearly and explicitly aware of the sensitivity of the data they process. For this reason, in various points of the web resource, this awareness is emphasized and the consequent attention that is dedicated to the processing of data from when they are collected until they are combined into a single global dataset, in which there is a high risk of cross-fertilization that could lead to the identification of individuals. From this attention it follows the interest in respecting some fundamental principles of data ethics, for example <b>transparency</b> (data sources are always mentioned, revisions and changes made, license, methods of data collection and so on) and <b>accountability</b> for the choices made in the analysis and processing of the data, such as k-anonymization.</p>

    <p><b>UNODC</b>: Although the DATAUNODC digital portal does not have sufficient specifications regarding the ethical and legal processing of data, the context in which the resource is inserted is that of UNODC, that is the branch of the United Nations that bases its work on ethical principles such as achieving <b>health, security and justice</b>. The Office is committed to supporting Member States in implementing the 2030 Agenda for Sustainable Development and the 17 Sustainable Development Goals (SDGs) at its core. The 2030 Agenda clearly recognizes that the rule of law and fair, effective and humane justice systems. The principles of <b>clarity and transparency</b> emerge from the organization's willingness to make the data collection methodology public. However, information regarding their process and treatment is scarce, and the non-liability statements on the conditions and updates of the data are not very precise and may suggest a slight disinterest in respecting ethical principles in the processing of data. For this reason, more detailed information on data processing and the ethical principles in processing it would complement that clarity in the statement of the work ethics guidelines of the United Nations Office on Drugs and Crimes.</p>

    <p><b>EU Commission and EU data portal</b>: One of the first value mentioned in different pages of the web resources of both the EU Commission and the EU Open Data Portal is transparency: the aim of collecting, processing and publishing open data for the general public to reuse is fundamental for the international organization, as it promotes both economic development within the EU and <b>transparency</b> within the EU institutions. It is not necessary to reiterate that the European Union is among the world leaders in promoting laws, regulations, methodologies and procedures for <b>data protection</b> (to name a few: The General Data Protection Regulation (GDPR) and the Data Protection Law Enforcement Directive). This attention to data protection is reflected and complements the aforementioned principle of transparency: the organization manages to balance these two values ‚Äã‚Äãthrough particular attention to methodological aspects, in order to facilitate the understanding of the data. In each report of the datasets used in the ODOHTEU project, the transparency of the methodology is guaranteed by the lengthy and detailed explanations regarding data sources, collection, metadata, and any other policy regarding the data process. The accuracy of the measurements and data reported in the datasets and reports is accompanied by statements of <b>liability and accountability</b>.</p>

    <p><b>World Bank Group</b>: the World Bank Group is one of the best institutions, among those selected by ODOHTEU project, in terms of interest towards data ethics and ethical behavior and integrity. Indeed, the Group has a specific department for this reason: The Bank Group's Ethics and Business Conduct Department (EBC), which promotes the development and application of the highest ethical standards by staff members. Firstly, in methods of collecting and processing data it is clear that the <b>human being is at the center</b> and each dataset is <b>bias-free</b>. Secondly, the high quality standards reflect in compliance with the principle of <b>sustainability</b>, as the data are characterized by integrity and timeliness. Furthermore, among the stated principls of the Organization we can read that one of their core principle is <b>professional integrity</b>, that is they develop and use <b>objective and transparent methods</b> to deliver reliable and trustworthy statistics and other products, based on professional principles and best practices. Last but not least, WBG guarantees <b>individual data control</b> as it is the only data owner and provider that has a specific technical mechanism for collecting request of deletion in case of personal data.</p>

    <p><b>OECD: Organisation for Economic Co-operation and Development</b>: The principles defining OECD‚Äôs work are <b>integrity, transparency and accountability</b>, as well as putting the <b>human being at the centre</b>, since the core mission of the organization is to build better policies for better lives. OECD goal is to shape policies that foster prosperity, equality, opportunity and well-being for all. The OECD also collaborates with governments, policy makers and citizens both in establishing evidence-based international standards and finding solutions to a range of social, economic and environmental challenges; and in designing and implementing policies by providing policy advice and recommendations on how to integrate these core principles into public sector reforms.</p>





    <h3>4.4 Technical analysis</h3>
    <p>TABBEEELLLAAA</p>
    <p>*provenance and URI are those of the Italy Country Profile dataset, but for ODOHTEU project have been used 28 Country Profile dataset, one for each member state of the European Union (until 2018, thus considering also UK). For reasons of brevity and to avoid redundancy, it was chosen to report only the URI and the provenance of the Italian state dataset, as the URIs and the provenance of the other datasets on the other European Union nations have the same syntactic and technical characteristics respectively.</p>


    <h2>5. Mash-up and final datasets</h2>
    <h3>5.1 Principles and goals</h3>
    <p>For the purposes of ODOHTEU project, as explained in the section "Scenario", the original datasets that we have found were not appropriate on several fronts:</p>
    <p>1) <b>technically</b>: almost half of the original datasets are in pdf format;</p>
    <p>1) <b>temporally</b>: we wanted to investigate data about the 21th century;</p>
    <p>2) <b>geographically</b>: we were interested only in the human trafficking happening in the European Union member states;</p>
    <p>3) <b>for their content</b>: having to choose some variables or indicators on which to focus our research, as regards the trafficking of human beings, we focused on the <b>total number of victims, sex, age or majority status, form of exploitation, citizenship and country of destination</b>; while for what concerns the growth or well-being indicators of the European states we have taken into consideration the <b>population growth, poverty rate, life expectancy, schooling/education, GDP, net migration, jobs, income, safety, health, environment, civic engagement, accessiblity to services, housing</b>.</p>
    <p>For all of these reasons we had to create new datasets out of the modification that proved necessary for the purpose of this project. Thus each ODOHTEU dataset has been created through a process of comparing and harmonizing existing data models of original datasets owners seeking coherency and consistency in data aggregation and manipulation.</p>

    <br>

    <p>In particular, in order to create ODOHTEU final mash up datasets we have decided to follow the <b>FAIR principles</b> stated by the Guidelines for Open Data provided by the EU: data have to be findable, accessible, interoperable and re-usable.</p>

    <p><b>Findable</b>: the first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services, so this is an essential component of the FAIRification process.</p>
    <p>F1. (Meta)data are assigned a globally unique and persistent identifier: both the mashed up data and the metadata we created according to the DCAT-AP are compliant with this point, presenting URI.</p>
    <p>F2. Data are described with rich metadata: we associated a rich amount of metadata compliant with the DCAT-AP specification, including not only all the mandatory classes with their respective mandatory properties but also some recommended and optional properties that were useful for our data.</p>
    <p>F3. Metadata clearly and explicitly include the identifier of the data they describe: for each dataset that is part of a catalogue and for our own dataset we associated to the metadata a unique identifier of the data described by means of the DCAT-AP optional property for datasets dct:identifier.</p>
    <p>F4. (Meta)data are registered or indexed in a searchable resource: All the data we used are identified by an URL that allows to access the source where they are registered. For the creation of the metadata associated with our data we used the DCAT-AP specification, whose aim is to enable a cross-data portal search for data sets and make public sector data better searchable across borders and sectors. Therefore, we can state that our (meta)data are registered in a searchable infrastructure.</p>
    <br>
    <p><b>Accessible</b>: once the user finds the required data, they need to know how can they be accessed.</p>
    <p>A1. (Meta)data are retrievable by their identifier using a standardised communications protocol: All the data we collected and mashed up and the relative metadata are retrievable through the HTTP or its extension HTTPS. Moreover, we provided also an explicit and clear contact protocol in the metadata by means of the names and emails of the data and metadata providers.</p>
    <p>A1.1. The protocol is open, free, and universally implementable: HTTP and HTTPS are compliant with these characteristics.</p>
    <p>A1.2 The protocol allows for an authentication and authorisation procedure, where necessary: the HTTP and HTTPS provide for authentication of the accessed website.</p>
    <p>A2. Metadata are accessible, even when the data are no longer available: metadata will remain accessible from the metadata web page of this web resource.</p>
    <br>
    <p><b>Interoperable</b>: the data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.</p>
    <p>I1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation: we used JSON and CSV for the representation of the mashed up data and RDF with the Turtle syntax to describe and structure the metadata.</p>
    <p>I2. (Meta)data use vocabularies that follow FAIR principles: the annotation format we used allow to use machine-readable terms from any controlled vocabulary. We used the ISO standard vocabulary to represent nations, the Linked Open Data vocabulary specification called DCAT-AP. These vocabularies are documented and resolvable using globally unique and persistent identifiers.</p>
    <p>I3. (Meta)data include qualified references to other (meta)data: JSON, CSV and the RDF schema account for the data exchange and cross reference among metadata.</p>
    <br>
    <p><b>Reusable</b>: the ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.</p>
    <p>R1. Meta(data) are richly described with a plurality of accurate and relevant attributes: our data and metadata are described through a rich and vary series of labels including the date of collection and modification of the data, the licence, the publisher, the creator, their content.</p>
    <p>R1.1. (Meta)data are released with a clear and accessible data usage license: ODOHTEU datasets are released under the Creative Common License CC BY 4.0, which is specified for the dataset and respective metadata we created.</p>
    <p>R1.2. (Meta)data are associated with detailed provenance: our project includes information about the provenance of data in a machine-readable format in the metadata codification. The website presents also a description of the workflow that led to final data.</p>
    <p>R1.3. (Meta)data meet domain-relevant community standards: we used the ISO standard for geographic information.</p>
    <br>
    <p>The principles mentioned above include three types of entities: data, metadata and infrastructure. Thus given this examination, we may claim that ODOHTEU research data complies with the principles of FAIR.</p>


    <h3>5.2 Changes to original datasets and creation of final ones</h3>
    <p>General overview on main changes and measures on original datasets in order to obtain ODOHTEU final mash-up datasets:</p>
    <p>TABEEEELLLAAA</p>

    <p>To obtain the final dataset, data manipulation was proven necessary. We have written and used python functions to extract data of interest, delete rows or columns of csv files, convert pdf files into csv files and join together data in order to create a new dataset in csv format. We also imported python libraries such as ‚ÄúCSV‚Äù and ‚ÄúCamelot‚Äù.</p>
    <p>Main functions used for data manipulation:</p>
    <pre style="margin-left: 2em; margin-right: 2em;">
      <code>
        import csv

        # process original dataset
        def process_metadata(metadata_file_path):
            data = []
            with open(metadata_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                reader.fieldnames = 'Country', 'Code', 'Education', 'Jobs', 'Income', 'Safety', 'Health', 'Environment', 'Civic engagement', 'Accessiblity to services', 'Housing', 'Housing1', 'Housing2'

                for row in reader:
                    data.append(row)
                return data


        processedFile = process_metadata('dataset.csv')

        # filter relevant data
        def do_filter(data, field, country):
            result = []
            CampiPresentiEuropa = set() # to know in advance which European Union countries are present

            for row in data:
                campoDaControllare = row[field]

                if campoDaControllare in country:
                    CampiPresentiEuropa.add(row[field])
                    result.append(row)
            return result


        EuropeFilter = do_filter(processedFile, 'Country',
                                 ["Italy", "Austria", "Belgium", "Bulgaria", "Crotia", "Cyprus", "Czech Republic", "Denmark",
                                  "Estonia", "Finland", "France", "Germany", "Greece", "Hungary", "Ireland", "Latvia",
                                  "Lithuania", "Luxembourg", "Malta", "Netherlands", "Poland", "Portugal", "Romania",
                                  "Slovakia", "Slovenia", "Spain", "Sweden", "United Kingdom", "Great Britain"])

        # create new dataset with data of interest
        def create_dataset(filter):
            with open('new_dataset.csv', 'w', newline='') as csvfile:
                fieldnames = filter[0].keys()
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for x in filter:
                    writer.writerow(x)


        create_dataset(EuropeFilter)
      </code>
    </pre>

    <h2>6. Sostenibility over time</h2>
    <p>ODOHTEU project, as well as its catalog and datasets, have been developed as the final examination for the Open Access and Digital Ethics course within the Master's Degree in Digital Humanities and Digital Information at the University of Bologna, and is therefore <b>not regularly managed and updated</b>. Also, <b>as it takes a picture of a particular moment of time, to be specific 2000-2018, we do not intend to change ODOHTEU</b>. However, to be able to make comparisons between datasets, it will be interesting to build new datasets for the following years, even because the datasets used for this catalog are preserved and continuously updated by the relevant institutions which own them. In any case, our scripts remain usable and can be rerun on new files at any time.  If anyone finds that  a new release of one of our input dataset is available, we will be happy to be told about it in order to update our automated script file. <b>Under CC 4.0.0, our scripts are authorised and licensed</b>.</p>
    <p>As regards the sustainability not over time but <b>qualitative</b> of the materials present in the ODOHTEU project, we have tried to maintain the historical series and above all its information load as a whole; we used persistent URIs; and we integrated the data with RDF metadata serialized in Turtle, using the DCAT standard integrated with other ontologies such as SKOS, FOAF, etc.</p>

    <h2>7. Visualization</h2>
    <p>What we were looking for or what we wanted to visualize clearly are <b>trends</b> (overall picture of data over time), <b>features</b> (some sample of overall data, as in the map visualization), <b>outliers</b> (some data points in the dataset), <b>similarity</b> (common features of data points).</p>
    <p>For achieving effective visualizations,  we tried to apply <b>redundancy of encoding</b>, by using color and shape for one dimension for instance, because it helps users‚Äô perception; and we used <b>familiar colors, icons, layouts</b> as visual hooks and reduce users‚Äô effort in understanding the intended message.</p>
    <p>We used different type of data visualization: mainly <b>static</b> but also <b>interactive</b> (e.g., if the user hovers the bar in the bar chart a tool-tip appears showing data); we also add an <b>animated</b> horizontal bar chart; as well as an interactive map (you can select the year you want to visualize data of).</p>
    <p>Finally, we also based our visualizations on the <b>type of data we wanted to represent</b>: categorical data, quantitative data or mixed data.</p>
    <p>Generally speaking, we decided which visualization was best suited to represent a dataset based on what we wanted to show. For instance, when we wanted to show <b>comparisons</b> we used bar charts and line charts. To show <b>composition</b>, we used pie charts to represent simple share of total.</p>
    <br>

    <h3>7.1 Visualization tools</h3>
    <p>In order to visualize the data the following libraries were used:</p>
    <p><a href="https://leafletjs.com/">Leaflet.js</a>: an open-source JavaScript library for mobile-friendly interactive maps which uses <a href="https://geojson.io/#map=2/20.0/0.0">GEOJSON</a>: Leaflet makes it possible to draw polygons directly from geojson files.
    <br>Code ¬© BSD <a href="https://github.com/Leaflet/Leaflet/blob/master/LICENSE"> https://github.com/Leaflet/Leaflet/blob/master/LICENSE</a>
    <br>Data ¬© OpenStreetMap contributors <a href="https://www.openstreetmap.org/copyright">https://www.openstreetmap.org/copyright</a></p>
    <p><a href="https://www.chartjs.org">Chart.js</a>: simple yet flexible JavaScript charting for designers and developers. License: <a href="https://www.chartjs.org/docs/latest/notes/license.html"> https://www.chartjs.org/docs/latest/notes/license.html</a></p>
    <p><a href="https://d3js.org/">D3.js</a>: a JavaScript library for manipulating documents based on data, which uses HTML, SVG and CSS; and combines powerful visualization components and a data-driven approach to DOM manipulation. Library released under BSD license.</p>


    <h3>7.2 Visualizations for each dataset</h3>
    <p>In order to visualize the datasets in a proper and consistent way, we have decided to develop:</p>
    <p>1) <b>multiple or simple bar charts</b>: often used with bivariate values, especially to combine a categorical value and a numeric value, and highlight differences between categories (their occurrence in the dataset). Indeed, we used it to represent data about age, sex and total number of victims both for datasets regarding 2015-2016 and datasets for 2017-2018, as well as for data from OECD dataset. An animated horizontal bar chart was used for display data from UNODC total detected victim dataset.</p>
    <p>2) <b>pie charts</b>: circles cut in segments to show parts and their proportion with respect to the whole, and to other parts. Used to show univariate dependant data. Indeed, we used it to represent datasets about 2013-2014 data on human trafficking and citizenship of victims reported in datasets of 2015-2016.</p>
    <p>3) <b>map</b>: as geographical maps can be more intuitive than other axes systems, we have decided to use this visualization to show the total population of each member state of the European Union in 2000, 2010 and 2018 (from the ODOHTEU dataset: Country profile for each member states of the European Union). This is a sample, as we can visualize in a map also the other indicators included in the dataset: poverty rate, life expectancy, school enrolling, GDP and net migration.</p>
    <p>4) <b>line plots as area charts</b>: used to show dependent data, usually between a categorical value and a numeric one (when changes occur in one variable, these affect the other variables), it‚Äôs a special type of line chart where areas divided show a comparison of variables and their dependency (e.g. inverse proportion) and of the ‚Äúsize‚Äù of a phenomenon. Indeed, we used it to represent data on forms of exploitation for the years 2017-2018.</p>

    



    <h2>8. RDF assertion of metadata</h2>
    <p>We provided data with its metadata in order to provide <b>effective reusable and interoperable data</b>, adopting the <b>DCAT AP version 2.0.0 specification</b>. We chose to include metadata for the entire catalogue, but also separately for each dataset. The RDF assertion for the metadata in <b>Turtle serialization</b> has been released in the metadata web page of this project website.</p>
    <p>The need for metadata in rdf derives from the goal of developing a project that was compliant with <b>LOD</b> characteristics. Indeed, in addition to having chosen to release ODOHTEU data sets in different <b>open and machine-readable formats</b>, we have specified the <b>license</b> in use for these datsets which derives from the one used in the original datasets reworked in the context of the ODOHTEU project. We used the RDF metadata to specify licence and attribution for each dataset. And therefore the third characteristic that allows to define "open" the data is the presence of <b>metadata that follows precise and internationally established standards or controlled vocabularies</b>, such as DCAT.</p>
    <p>Following the <b>5-STAR OPEN DATA MODEL</b>, we have made available our datasets under a specified licence, making it available as <b>structured data (xml, csv, json)</b> not bound to specific software or a specific vendor. Then we used meaningful URIs to denote datasets (e.g. <code>data/id/2015-16EU-citizenship.zip</code>) and triples‚Äô objects (e.g. <code>http://publications.europa.eu/resource/authority/file-type/CSV</code>). By "meaningful" we refer to the obligation of ensuring that <b>URIs are persistent, dereferenceable and unambiguous</b>; they should be supported by a <b>reliable infrastructure</b>; and they should follow the pattern:<code>http: // {domain} / {resource-type} / {concept} / {reference}</code></p>



    <h2>9. Conclusions</h2>
    <p>The data we have collected, manipulated and visualized on human trafficking help shed light on a global phenomenon, analyzed here for the member states of the European Union, but by crossing and comparing datasets with different original sources: from international organizations to those based overseas, to the data of the European Union itself. One of our goals from the beginning has been to have a look without bias or prejudices on the phenomenon, so also for this reason we have chosen such different sources for the ODOHTEU datasets. <b>Comparing the data on this crime with those relating to the well-being and social and economic growth of a country can allow us to get a more precise idea of the phenomenon and the possible correlations with the well-being of a nation, to overcome prejudices and seek effective strategies to fight a crime against human beings and their fundamental rights.</b></p>

  </body>

</html>